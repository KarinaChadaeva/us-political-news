{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С сайта Breitbart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dtp\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.breitbart.com/politics/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (compatible; ResearchBot/1.0; +https://example.com/bot)\"\n",
    "    )\n",
    "}\n",
    "CUTOFF = datetime(2025, 6, 1, tzinfo=timezone.utc)\n",
    "\n",
    "\n",
    "def fetch_page(page: int) -> BeautifulSoup:\n",
    "    url = BASE_URL if page == 1 else f\"{BASE_URL}page/{page}/\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def parse_article(card):\n",
    "    h2 = card.find(\"h2\")\n",
    "    if not h2 or not h2.a:\n",
    "        return None\n",
    "    title = h2.get_text(strip=True)\n",
    "    url = urljoin(BASE_URL, h2.a[\"href\"])\n",
    "\n",
    "    time_tag = card.find(\"time\")\n",
    "    if not time_tag or not time_tag.has_attr(\"datetime\"):\n",
    "        return None\n",
    "    pub_dt = dtp.isoparse(time_tag[\"datetime\"])\n",
    "\n",
    "    author_tag = card.find(\"address\")\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"pub_date\": pub_dt.isoformat(),\n",
    "        \"author\": author,\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_pages(start_page=1, end_page=5):\n",
    "    results = []\n",
    "\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        try:\n",
    "            soup = fetch_page(page)\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Failed to load page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "        articles = soup.select(\"section.aList article\")\n",
    "        page_data = []\n",
    "\n",
    "        for art in articles:\n",
    "            data = parse_article(art)\n",
    "            if not data:\n",
    "                continue\n",
    "\n",
    "            pub_dt = dtp.isoparse(data[\"pub_date\"])\n",
    "            if pub_dt < CUTOFF:\n",
    "                print(f\"⚠️  Reached cutoff date on page {page} — stopping.\")\n",
    "                return results + page_data\n",
    "\n",
    "            page_data.append(data)\n",
    "\n",
    "        if page_data:\n",
    "            last_date = min(dtp.isoparse(item[\"pub_date\"]) for item in page_data)\n",
    "            print(f\"✅ Page {page}: {len(page_data)} articles, last date: {last_date.date()}\")\n",
    "        else:\n",
    "            print(f\"⚠️  Page {page}: no valid articles found.\")\n",
    "\n",
    "        results += page_data\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_csv(rows, path=\"breitbart_politics_2025.csv\"):\n",
    "    fieldnames = [\"title\", \"url\", \"pub_date\", \"author\"]\n",
    "    with open(path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        if f.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_PAGE = 26\n",
    "END_PAGE = 30\n",
    "\n",
    "data = crawl_pages(START_PAGE, END_PAGE)\n",
    "save_csv(data)\n",
    "print(f\"\\nDone! Saved {len(data)} articles from pages {START_PAGE}–{END_PAGE}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE = \"breitbart_politics_2025.csv\"\n",
    "OUTPUT_FILE = \"breitbart.csv\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (compatible; ResearchBot/1.0; +https://example.com/bot)\"\n",
    "}\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        content_div = soup.find(\"div\", class_=\"entry-content\")\n",
    "        if not content_div:\n",
    "            return \"\"\n",
    "\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"[!] Failed to fetch {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def enrich_with_text(input_path, output_path):\n",
    "    with open(input_path, newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        rows = list(reader)\n",
    "\n",
    "    enriched = []\n",
    "    for i, row in enumerate(rows, 1):\n",
    "        url = row[\"url\"]\n",
    "        print(f\"[{i}/{len(rows)}] Fetching: {url}\")\n",
    "        article_text = extract_article_text(url)\n",
    "        row[\"text\"] = article_text\n",
    "        enriched.append(row)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    fieldnames = list(enriched[0].keys())\n",
    "    with open(output_path, \"w\", newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(enriched)\n",
    "\n",
    "    print(f\"\\nSaved {len(enriched)} articles to {output_path}.\")\n",
    "\n",
    "\n",
    "enrich_with_text(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С сайта Mother Jones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.motherjones.com/politics/\"\n",
    "STOP_PATTERN = re.compile(r\"/2025/04/\")\n",
    "HEADERS = {\"User-Agent\": (\"Mozilla/5.0 (compatible; ResearchBot/1.0; +https://example.com/bot)\"),}\n",
    "OUT_PATH = \"motherjones_politics_2025.csv\"\n",
    "REQUEST_DELAY = 1.5\n",
    "MAX_PAGES = 30\n",
    "\n",
    "\n",
    "def fetch_listing(page: int) -> BeautifulSoup:\n",
    "    url = BASE_URL if page == 1 else f\"{BASE_URL}page/{page}/\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        content_div = (\n",
    "            soup.find(\"div\", class_=\"article-body\")\n",
    "            or soup.find(\"div\", class_=\"element-article-body\")\n",
    "            or soup.find(\"article\")\n",
    "        )\n",
    "        if not content_div:\n",
    "            return \"\"\n",
    "\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        return \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch article {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def crawl(start_page: int = 1, max_pages: int = MAX_PAGES) -> list[dict]:\n",
    "    results: list[dict] = []\n",
    "    stop = False\n",
    "\n",
    "    for page in range(start_page, max_pages + 1):\n",
    "        if stop:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            soup = fetch_listing(page)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        cards = soup.select(\"li.article-item, li.top-article-item\")\n",
    "        if not cards:\n",
    "            print(f\"Page {page}: no article cards found.\")\n",
    "            break\n",
    "\n",
    "        for card in cards:\n",
    "            h = card.find([\"h2\", \"h3\"])\n",
    "            if not h or not h.a:\n",
    "                continue\n",
    "\n",
    "            url = urljoin(BASE_URL, h.a[\"href\"])\n",
    "            if STOP_PATTERN.search(url): # дошли до апреля 2025\n",
    "                stop = True\n",
    "                print(\"Stop pattern reached — ending crawl.\")\n",
    "                break\n",
    "\n",
    "            title = h.get_text(strip=True)\n",
    "            text  = extract_article_text(url)\n",
    "\n",
    "            results.append({\"title\": title, \"url\": url, \"text\": text})\n",
    "\n",
    "        print(f\"Page {page}: collected {len(results)} articles\")\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_csv(rows, path=OUT_PATH):\n",
    "    if not rows:\n",
    "        print(\"Nothing to save.\")\n",
    "        return\n",
    "\n",
    "    fieldnames = [\"title\", \"url\", \"text\"]\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"\\nSaved {len(rows)} articles to {path}.\")\n",
    "\n",
    "data = crawl(start_page=1, max_pages=MAX_PAGES)\n",
    "save_csv(data, OUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем статьи Common Dreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://www.commondreams.org/politics\"\n",
    "STOP_PATTERN = re.compile(r\"/2025/04/\")\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; ResearchBot/1.0; +https://example.com/bot)\"}\n",
    "OUT_PATH = \"commondreams_politics_2025.csv\"\n",
    "REQUEST_DELAY = 1.5 \n",
    "MAX_PAGES = 40 \n",
    "\n",
    "\n",
    "def fetch_listing(page: int) -> BeautifulSoup:\n",
    "    url = BASE_URL if page == 0 else f\"{BASE_URL}?page={page}\"\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        scripts = soup.find_all(\"script\", type=\"application/ld+json\")\n",
    "        for script in scripts:\n",
    "            try:\n",
    "                data = json.loads(script.string)\n",
    "                if isinstance(data, dict) and data.get(\"@type\") == \"NewsArticle\":\n",
    "                    return data.get(\"articleBody\", \"\").strip()\n",
    "                # если это граф\n",
    "                if isinstance(data, dict) and \"@graph\" in data:\n",
    "                    for item in data[\"@graph\"]:\n",
    "                        if item.get(\"@type\") == \"NewsArticle\":\n",
    "                            return item.get(\"articleBody\", \"\").strip()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        print(f\"Не найден articleBody в {url}\")\n",
    "        return \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке текста: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def crawl(max_pages=MAX_PAGES) -> list[dict]:\n",
    "    results: list[dict] = []\n",
    "    stop = False\n",
    "\n",
    "    for page in range(0, max_pages):\n",
    "        if stop:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            soup = fetch_listing(page)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        # каждая новость лежит в <article>\n",
    "        cards = soup.select(\"article\")\n",
    "        if not cards:\n",
    "            print(f\"Page {page}: no article cards found.\")\n",
    "            break\n",
    "\n",
    "        for card in cards:\n",
    "            h = card.find([\"h2\", \"h3\"])\n",
    "            if not h or not h.a:\n",
    "                continue\n",
    "\n",
    "            url = h.a[\"href\"]\n",
    "            if not url.startswith(\"http\"):\n",
    "                url = urljoin(BASE_URL, url)\n",
    "\n",
    "            if STOP_PATTERN.search(url): # дошли до апреля 2025\n",
    "                stop = True\n",
    "                print(\"⚠️  Stop pattern reached — ending crawl.\")\n",
    "                break\n",
    "\n",
    "            title = h.get_text(strip=True)\n",
    "            text  = extract_article_text(url)\n",
    "\n",
    "            results.append({\"title\": title, \"url\": url, \"text\": text})\n",
    "\n",
    "        print(f\"Page {page}: collected {len(results)} articles\")\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_csv(rows: list[dict], path: str = OUT_PATH) -> None:\n",
    "    if not rows:\n",
    "        print(\"Nothing to save.\")\n",
    "        return\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"url\", \"text\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"\\nSaved {len(rows)} articles to {path}.\")\n",
    "\n",
    "\n",
    "data = crawl(MAX_PAGES)\n",
    "save_csv(data, OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://truthout.org/section/politics-elections/\"\n",
    "PAGE_URL = BASE_URL + \"page/{page}/\"\n",
    "\n",
    "STOP_PATTERN = re.compile(r\"/2025/04/\")\n",
    "MONTHS_WANTED = {\"2025/06\", \"2025/05\"} \n",
    "\n",
    "HEADERS = {\"User-Agent\": (\"Mozilla/5.0 (compatible; ResearchBot/1.0; +https://example.com/bot)\")}\n",
    "\n",
    "OUT_PATH = Path(\"truthout_politics_2025_may_june.csv\")\n",
    "REQUEST_DELAY = 1.5\n",
    "MAX_PAGES = 40\n",
    "\n",
    "\n",
    "def fetch_listing(page: int = 1) -> BeautifulSoup:\n",
    "    url = BASE_URL if page == 1 else PAGE_URL.format(page=page)\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "            try:\n",
    "                data = json.loads(script.string)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                continue\n",
    "\n",
    "            # Иногда JSON обёрнут в \"@graph\"\n",
    "            if isinstance(data, dict) and data.get(\"@type\") == \"NewsArticle\":\n",
    "                return data.get(\"articleBody\", \"\").strip()\n",
    "\n",
    "            if isinstance(data, dict) and \"@graph\" in data:\n",
    "                for item in data[\"@graph\"]:\n",
    "                    if item.get(\"@type\") == \"NewsArticle\":\n",
    "                        return item.get(\"articleBody\", \"\").strip()\n",
    "\n",
    "        # fallback — если по какой-то причине JSON не нашёлся\n",
    "        article = soup.find(\"article\")\n",
    "        if article:\n",
    "            paragraphs = article.find_all(\"p\")\n",
    "            return \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "\n",
    "        return \"\"\n",
    "    except Exception as exc:\n",
    "        print(f\"Failed to fetch “{url}”: {exc}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def crawl(max_pages=MAX_PAGES) -> list[dict]:\n",
    "    collected: list[dict] = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "\n",
    "        try:\n",
    "            soup = fetch_listing(page)\n",
    "        except Exception as exc:\n",
    "            print(f\"Listing page {page} failed: {exc}\")\n",
    "            break\n",
    "\n",
    "        cards = soup.select(\"article\")\n",
    "        if not cards:\n",
    "            print(f\"Page {page}: никаких <article> не нашли, стоп.\")\n",
    "            break\n",
    "\n",
    "        for card in cards:\n",
    "            h = card.find([\"h2\", \"h3\"])\n",
    "            if not h or not h.a:\n",
    "                continue\n",
    "\n",
    "            url  = h.a.get(\"href\") or \"\"\n",
    "            url  = url if url.startswith(\"http\") else urljoin(BASE_URL, url)\n",
    "            path = urlsplit(url).path\n",
    "\n",
    "            if STOP_PATTERN.search(path):\n",
    "                print(\"Stop-pattern reached → crawl finished.\")\n",
    "                return collected\n",
    "\n",
    "            # фильтруем только нужные месяцы\n",
    "            month_part = \"/\".join(path.split(\"/\")[:3])\n",
    "            if month_part[1:] not in MONTHS_WANTED:\n",
    "                continue\n",
    "\n",
    "            title = h.get_text(strip=True)\n",
    "            text  = extract_article_text(url)\n",
    "            collected.append({\"title\": title, \"url\": url, \"text\": text})\n",
    "\n",
    "        print(f\"Page {page}: всего статей собрано {len(collected)}\")\n",
    "        time.sleep(REQUEST_DELAY)\n",
    "\n",
    "    return collected\n",
    "\n",
    "\n",
    "def save_csv(rows: list[dict], path=OUT_PATH):\n",
    "    if not rows:\n",
    "        print(\"Nothing to save.\")\n",
    "        return\n",
    "\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"url\", \"text\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"\\nSaved {len(rows)} articles → «{path}»\")\n",
    "\n",
    "\n",
    "data = crawl()\n",
    "save_csv(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
